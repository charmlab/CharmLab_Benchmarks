architecture: "mlp" # Options: linear, mlp, tree, forest
backend: "pytorch"  # Options: pytorch, xgboost
cache_dir: "./.model_cache"
load_pretrained: false # Toggle to load cached weights instead of training

epochs: 100
batch_size: 1000 # make this the same as dataset size if you want to train on the whole dataset at once
learning_rate: 0.001
n_output: 1
optimizer: "adam" # Options: adam, sgd, rms
loss_function: "BCE" # Options: BCE, MSE, etc
hidden_layers: [[50, 100], [100, 200]] # Specific to MLP
tree_estimators: 100    # Specific to Forest/XGBoost
subsample: 0.9          # Specific to XGBoost
output_activation: "sigmoid" # Options: sigmoid, softmax, none (for regression)